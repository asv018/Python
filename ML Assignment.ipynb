{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "# can they be mitigated?\n",
    "\n",
    "# Overfitting: When the model is too complex, it will fit the training data very well, but it will not generalize well.\n",
    "# It will perform very well on the training data, but it will not perform well on the test data.\n",
    "# Underfitting: When the model is too simple, it will not fit the training data well, and it will not generalize well.\n",
    "# It will perform poorly on both the training data and the test data.\n",
    "# Overfitting can be mitigated by using regularization, which constrains the weights of the model.\n",
    "# Underfitting can be mitigated by using a more complex model, or by reducing the constraints on the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "# Overfitting can be reduced by using regularization, which constrains the weights of the model.\n",
    "# It can also be reduced by using a simpler model, or by increasing the size of the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "# Underfitting occurs when the model is too simple, and it does not fit the training data well.\n",
    "# It can occur when the training data is too small, or when the model is too constrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "# variance, and how do they affect model performance?\n",
    "\n",
    "# The bias-variance tradeoff is a fundamental concept in ML. It represents a tradeoff between model simplicity and accuracy.\n",
    "# Bias refers to errors due to overly simplistic models (underfitting).\n",
    "# Variance is the error due to overly complex models (overfitting).\n",
    "# High bias leads to systematic errors, while high variance leads to errors due to model sensitivity to variations in data.\n",
    "# Optimal model performance lies in balancing bias and variance to minimize total error. Too much bias leads to underfitting, and too much variance leads to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "# Common methods:\n",
    "# Visual inspection of learning curves.\n",
    "# Cross-validation to estimate model performance on unseen data.\n",
    "# Comparing training and validation (test) set performance.\n",
    "# Monitoring model complexity and adjusting it.\n",
    "# Analyzing feature importance.\n",
    "# You can determine overfitting by observing a significant performance gap between the training and validation (test) data. Underfitting is evident when both training and validation performance are poor.\n",
    "\n",
    "# Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "# Bias: High bias models are overly simplistic and underfit. Examples include linear regression on complex nonlinear data.\n",
    "# Variance: High variance models are overly complex and overfit. Examples include deep neural networks on small datasets.\n",
    "# High bias models have poor performance on training and test data, while high variance models have good training but poor test performance.\n",
    "\n",
    "# Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
    "\n",
    "# Regularization is a technique used to prevent overfitting by adding a penalty term to the model's loss function.\n",
    "# Common regularization techniques:\n",
    "# L1 Regularization (Lasso): Adds the absolute values of coefficients to the loss. Encourages sparsity.\n",
    "# L2 Regularization (Ridge): Adds the squares of coefficients to the loss. Encourages small weights.\n",
    "# Dropout (for neural networks): Randomly deactivates neurons during training.\n",
    "# Early stopping: Stops training when validation error starts to increase.\n",
    "# Regularization discourages models from becoming too complex, helping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
